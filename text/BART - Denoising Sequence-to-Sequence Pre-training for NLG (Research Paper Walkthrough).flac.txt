Hi there, welcome to the new video. Interesting paper that came out. Just dropped RGB t a and became really popular its title as Bart V node in sequence. Sequence Professor training for natural language. In addition translation and comprehension help from researchers. From Facebook, AI, Sol headboard stats for byte direction or two regression of Transformers. So how's it is all digital multi tool bits, Arden wave in sequence to sequence Bayes architectures and in this fabled Imaging to talk about photos of routing skin, their daughters applied. We present Bart or do you know he's involving code for putting Sigma sequels model screen, byte corrupting a text with some algebra noise function and also learning to reconstruct those neural text. Okay. So, let me tell you a bit about how he knows more than Cody works. So, let's consider some dance, architectures bits. If you have three layers than two, and then again, we have three units at this place. So, this is the bottleneck layer that we have and it's, if you had three inputs, I want to AI 3, which go with her hair and I doubt putting, we should the same values over here again, which means we want to reconstruct these input values and output. So the more than all needs to optimize on an encoding says that the tour Twitter or the bottleneck layer wave, certain dance at optimize and compressed in such fashion servers that. No, because he uses that representation to upscale those values to the same dimension of the input. It should be able to do that without any loss of information so that he could Ram once you get some values. Let's say I have One, I had to, I had three you calculate the loss over here, which could be a pharmacy or if it's a classification kind of a setting, then you can have crossed and repeat and CNN values. Get property back, were storing the wave surf board decoder. An encoder. That's how it works. Not talking about this in a sequin sequin setting, which is one of the things web, art is applied. So let's talk about God, or lstms CD CDs at 3 units. And these are three words that go into this. How you get the tart with the, let's call it as tight as you have new units, which is a decoder in. So this talk with them is going to go. So wave hair. You have a sequence, starting token of this produces a output open. This goes over here, distribution of output order. This goes over there. This police as output or three servers old is Kris Kross output, contain pork and packet. Lecture concert in the digit for thinking ratio is equal to 0. Bits means, we always consider that output that web page and read our previous step as the input to my next step. This could be a grid off and you can have also have actual and butchering over here, CD, out of you. And these were you, I want to and I tree another loss. Would be calculated says that because these web my imports, which means my ground router zaiwa should be here by 2, should be here and I too should be here. So he's always there to see, right. There's nothing, but the Oghma Chi song. Shoot that you applied to a soft, Max vocabulary, activity, training stats. So once I get these 41 or 23, I already know what it should have produced which is I want to try 3 at a cross entropy loss. I calculate that lost its quarter task, Lan and back Roc, Roc a training than codec and Dakota Waits. So that's how the dinos in Auto encoding bits. He works in a sequence to sequence architectures. You but yeah, this is obviously a listing in there, talk about doing a stock classes, units, which will see in a moment. And on top of this, they propose some free training objectives. So you might already know that. Thinking and fine-tuning. Is the new paradigm in natural language processing to help. People are usually walking with sport example, of a case of board. You have this much language model in case of Pegasus, you have Gap sequence predictions in case 55, you have spanned predictions on and so on and so forth. Sub Roc proposes, some different type of objectives which helps it to generalize well for translation language in relations kind of task. So web. Let's go forward.  Swith bored. As you already know what is just the encoding part of the entire Transformer Sigma Sigma is architecture with the preaching of jpeg on web servers training was Eminem and NSP where Eminem's Stan squared Moss language, model e, n, n s p stand for Mexicans predictions about Eminem. Where the ideas little you have a sequence of a b, c d, e, c model, some random samples and digit word from the input, which is wrong with this damn short hair and a dog, couldn't you just predictor those words Bayes on other worlds that occur in the same context, and it's bad relations, which means, at any point, even, and the safety, to ever be able to observe a CNA and similarity. So, we'll be able to observe a CNA and it's not sequential in nature, which means you'll first. B, and then you can use that be all to ask on task for Tuesday. That's not how it works. B&D about to do simultaneously which means both of them cannot return to each other.  In the important as well. So talking about Djibouti now, which is altered against human nature, which means at any * 50, but said, the sea point, the model should be able to produce. See, my only thinking things that happened to be in the past, which means it can see the start to walking in a and simile for Anytime, Sublette City to, which could be eat at this point. If the model is supposed to produce anything, it will be able to just speak the words left, whatever has been inputted, but just start walking a b and c and that is single in this direction, which means it is just you model Richmond. And there's no point. It can't be too. It's right. So that's one major difference between board in Jeopardy as well, which makes them different in terms of the generation abilities. So not talking about bart.  Says, we all do so bored and you pretty one node sequence, sequence architectures, but it was only in quarter and Judy was only decoder. Now, here we have both encoding and decoding blocks. So let's see if I do find noise function, fight metrics and some input and output some iHeart, which is a noisy in port side would have given this a b c, d and e and the task of fires term, Austin Ram, running boards and the output would be a PDF file. So that is my iHeart know, this goes to encoder and columns that representation of model stochastic and North muskegon's a producer stats Hardware over. There is Ghost Protocol that which is all to regression of nature, which is RGB this time. Now. The output end of this decoder is supposed to lose all the tokens over here which includes the master has been. So now you can see it right? That wave. The model is also learning to generate sequence of words in order digital fashion. Where is it is also learning to PDF some models to kill switch web there and then put in  Cell, it's going forward.  Wave Wireless in this model, that is just recap, Saturday nodes, in order encoder that Maps graph to document to original document that it was derived from its implemented in a sequence. Sequence, model fashion with a byte Direction encoding boxes of birthday and Lyft ride or to regression Dakota images of chi PDF style. Victor is to optimize on the negative log likelihood on the original document servers something to be avoided discussing. While I was discussing the ls team example, with the loss of supposed to maximize on the likelihood, which is like probably of seeing and output o, which is the sequence of old ones or okay, give in the input eyes. With the gauntlet is also all the lies that you have and then you calculate discussing trophy between the size and all the others that you have. Okay, let's move forward.  No, talking with the main part of Bart, which is the programming objective. So, as we saw sensitivity nodes, in order to encoder, which means we'll have some transformation function that transforms the input by adding some noise, the noise would be of different types. And finally, getting annoyed version of the input, which goes to the encoder and, and the decoder you training against the node noise version. So, that is that propagates backwards, some of that CD list of functions that will Define the first is token masking. So the same as what bird did they randomly sample term word from the airport and replace. Those were with this must open and output until you produce a, what word has to be pressing against his master Chi, which is the output of the decoder. I'm like bored with output was at the encoding entities self. The second function is to Convention. Will you delete someone to consult with the model? Supposed to produce a web, the undiluted version of her? So this is it harder task for the model. If you see, because under the mask  You will give him some positional inputs already URL in the position where we see the Moscow, but here I haven't produced any must open. So let's see if my input as a b c d and e is close to my fire function and my fire pixel 1 Ram node function with the lecture to open relations. At this point. It will produce a CD and he or it could be just like Cas a DNA. So what you were finally selected, one of them goes to my dick colder and my model samples could use a b c, d and e node should already know. Like he was not there and see was not there. Although I haven't given any model stoken, which would help supervised. My recorded node exact position. We're trapped inside those things, in this case model supposed to guess by itself. What should be the correct output? That is a second dust which is talking religion. And if you want pixel support this function, you can think of removing all the dogs from then put in the model is supposed to produce the entire sequence with Dan empty input.  Which is the hardest task of the model can do, okay?  The Nexus 6 in feeling, so they have figured I thinking about ya. So, can we have seen? Like you must be and do your hair and model supposed to produce the entire thing? Then we had to CD yishun web. You don't give the position where you deleted the things which are represented by these High phones or dashes and model is still supposed to produce and I input the next posting text in feeling. So here you essentially Mars group of words with a single Mast Oakland. And also, these Muslims could be different. So this is also inspired from spanberg. So let's see how it's all. So different from somebody were doing a master plan, for every word, Verizon text in feeling or spanberg ushe. Let it choose two words. If the speed limit is two and replace them with a single model. Can you add an output into multiple for two different words? You give one single Mast, who can for the entire span? So that is what takes inflamed us, but the span linking will also be  Acne node affect, which was, I think the case in Spanish. So that is a total transformation node. Talkin about Cynthia's permutation where the ideas for Shuffle lexical units of the input, which you are interested. In that case, you would produce output and input everything at the word label, which is that token. See you do the shopping at the token label. So for a b c, d e, and to do this then put that could be created. This goes to my model at the encoder and decoder ring screen against the actual input, which was a b c. D. And that is what model is training against for the wave of encoding and decoding. And it should be able to produce actual and put a couple of vibrations in the last transformation function that they propose is called documentation, whether it was the document saving proposed ABC, nothing that goes to encoder for bcba, but you would expect the model to produce a b, c added, a quarter output. These are all the transmission functions that the board proposes bits.  Challenges the model to really learn different aspects of language and visual birth to any kind of transformation that it could face. And this wave, the model is also supposed to generalizes. Well, okay, let's move forward.  Yeah, that was the preaching for but then the test out all the fine-tuning stuff with the trailer with his classification tasks, such a secret classification into can classification and the ultra tasks the term Novation abilities with machine translation.  Yeah, I think. Now we're done with the paper.  Ai Se for lexical term, make sure to hit that like button and subscribe to the channel. Also shared across the difference to whosoever's interested in such constant. I'll meet you in the next one but by and take care. 