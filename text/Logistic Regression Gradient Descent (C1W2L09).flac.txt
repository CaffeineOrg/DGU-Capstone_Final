Welcome back in this video will talk about how to compute the rivet is for you to inference and gradient descent for the chi lecture of Russian. The chi takeaways will be what you need to intimate. That is the chi equation you need in order to implement gradient descent Photoshop linearregression, but I want to do this computation using the computation graph. Graph is a little bit of an overkill for deriving. Gradient descent full digits of regression, but want to start explaining things, this way to get you to Millie with these ideas. So that hopefully will make a bit more sense when we talked about. Your networks. Let's dive into gradient, descent for logistic regression. Heavy Chi. If we had set up, logistic regression as follows, your predictions, y hats is defined as follows where Z is that. And if we focus on just one example for now, then the loss or expected, that one example is defined as follows. Where is the output of logistic regression? And why is the ground truth label? So let's write this out as a computation draft. And for this example, that's a we have only two features X1 and X2. So in order to compute do you will need to input w1w to has be in addition to the Future values affect the next to. So these things in a competition golf can't use the computers. You witches W 1 x 1 + W, 2 x 2, + B, draw a tackle box around that and then we can shoot y hats or a equals 6, Mugsy. That's the next step in the computation draw. And then finally we compute l a y and I won't copy the phone with the game. So in logistic regression, what we want to do is to modify the parameters, w&b in order to reduce this loss, with this tribe, the four properties. In steps of how you ask, you compute. The lost on a single training is awful. Now, let's talk about how you can. Go backwards to talk to compute stats, zeros has she has a cleaned-up version of the diagram because what we want to do is compute the riveters respect to this loss. The first thing we want to do and going back with stats, to compute the derivative of this loss with respect to the script over there. And we respect to the zeroth Olay and sew-in in the cold, you know, you just use the AI right to denote this variable. And it turns out that if you are familiar with Calculus, you could show that this ends up being next to the y, + 1 - y over. 1 - 8 and the way you do that, if you take the formula for the loss and if you have to live in calculus, you can compute the derivative for this packet to the airport lowercase Ai. And you get this phone's up. But if you're not, then your Calculus don't worry about it. We will provide the derivative formulas, you need throughout the store. So if you're thinking how cool is URL, encourage you to look up the formula for the lost from their previous slide in traffic and director for this recipe using, you know, calculus, but if you don't know enough. Don't worry about it. Now, having computed just going to the DA is a derivative of help available respect to AI. You can then go backwards and it turns out that you can show Daz which this is the python code variability names is going to be the derivative of the lost respect to D4L. You can video bytes are the loss including and why explicit Prime term for not eating either. Notation is equally acceptable feature show, that doesn't equal to a minus. Why just a couple comments only for those of you to expect in Calculus. If you're not explain Chi versus don't worry about it. That turns out that this right d l t, z just can't be stressed as the l, d, a x t, a d z. And it turns out that the ADC, this turns out to be a * 1 - 8 and d l g a b. A previously worked out over here. And so if you take these two quantities, you do you how do you say this term together with d a d z which is this term. And just take these two things that multiply them, you can show that you to the equation, simplifies 28 - why. So that's how you do the chain rule? That I breathe the entities into F1. So feel free to go through that calculation yourself, if you URL knowledge, multi calculus, but if you aren't all, you need to know, is that you can come shoot, these. That's a - Wireless already done that calculus for you. And then the final step in backpropagation is to go back to compute how much you need to change W and Pete. So into thinking you can show that the derivative Moritz packet to W1 and encoding will call this dw1. That this is equal to x 1 x DC and then some of these ew to me how much you want to change. W2 is x 2 x d, c & P, C CD be equal to DC. So if you want to do gradient descent will respect. You just as one example of what you would do is to follow. You would use this formula to compute easy and then use these formulas to compute, dw1, gw2 and DP. And then you perform these updates. Everyone gets updated. Tell me one. Mine is linear Alpha times digital e1w to get updates, a similarity and be getsetaz. B- to learning rate, x, d, PDF to be one step of grid affect to a single example. So, you seen how to compute derivative and Implement gradient descent for logistic regression with respect to a single training sample, but do training linearregression model. Do you have not just one training sample given Thai training, stats of Emma training examples. So in the next video, let's see how you can take these ideas and apply them to learning. Not just from one example, Performance, I training set. 