Hurricane Florence came by while I was working on stats Quest, dark clouds filled the sky, but that didn't stop stats Quest, stats squared.  Hello, I'm just armor, and welcome stats Quest. Today. We're going to be talking about some machine learning, fundamentals, bias, and variance. And they're going to be clearly explained.  Imagine we measured the weight and height of a bunch of mice and plotted the data on a graph.  Light my skin to be short.  And heavier, mice tend to be taller.  But after a certain weight mice, don't get any taller just more obese.  Given the stator, we would like to predictor Mouse height, given its weight. For example, if you told me your mouse weigh this much.  Then we might predict that the mouse is this tall.  Ideally, we would know the exact mathematical formula that describes the relationship between weight and height. But in this case, we don't know the formula. So we're going to use to machine learning methods to approximate this relationship.  However, I'll leave the true relationship curve in the figure for reference.  The first thing we do is split the data into two sets, one for training in machine learning algorithms and one for testing them.  The blue dots are the training set and the green dots are the testing set.  Here's just the training set.  The first machine learning algorithm that we will use is linearregression AKA least-squares.  Linear regression bits, a straight line to the training set.  Note, the straight line doesn't have the flexibility to accurately. Replicate The Ark in the true relationship.  No matter how we try to fit the line. He will never curve.  The straight-line will never capture the true relationship between weight and height. No matter how well we fit it to the training set.  The inability for a machine-learning method like linearregression to capture. The true relationship is called by us because the straight line can't be curved like the true relationship. It has a relatively large amount of python AI us.  Another machine learning method, might fit a squiggly line to the training set.  The squiggly line is super flexible and hugs. The training set along the archive, the true relationship.  Because the squiggly line can handle the Ark in the true relationship between weight and height. It has very little bias.  We can compare how well the straight line in the squiggly line. Hit the training set by calculating their sums of squares.  In other words, we measure the distances from the fit lines to the data squared them and add them up. They are squared, so that negative distances do not cancel out, positive distances.  Notice how the squiggly line fits the data. So well that the distance is between the line and the data are all 0.  In the contest to see whether the straight-line fits the training set better than the squiggly line. The squiggly line winds.  But remember so far, we've only calculated the sums of squares for the training set.  We also have a testing set.  Now, let's calculate the sums of squares for the testing set.  In the contest to see whether the straight-line fits the testing set better than the squiggly line.  The straight line winds.  Even though the squiggly line did a great job hitting the training set. It did a terrible job, hitting the testing set.  Ai and machine learning lingo. The difference in fits between dataset is called variances.  The squiggly line has lobias since it is flexible and can adapt to the curve in the relationship between weight and height.  But the squiggly line has high variability because it results in vastly different sums of squares for different data sets.  In other words, it's hard to predict how well the squiggly line will perform with future dataset can might do well sometimes. And other times it might do terribly.  In contrast, the straight line has relatively High by us since it cannot capture the curve in the relationship between weight and height, but the straight line has relatively low variance because the sums of squares are very similar for different data sets.  In other words, the straight-line might only give good predictions and not great predictions, but they will be consistently. Good predictions, bam.  Oh, no, terminology alert, because the squiggly line fits the training set really well, but not the testing set. We say that the squiggly line is over fit in machine learning. The ideal algorithm has Low by us and can accurately model the true relationship.  And it has low variability by producing consistent predictions across different data sets.  This is down by finding The Sweet Spot between a sample model and a complex model.  Oh, no, another terminology Alert 3. Commonly used methods for finding the Sweet Spot between simple and complicated model Zar regular ization boost and a bagging. The stats Quest on random Forest. Show an example of bagging in action and we'll talk about regularization and posting in future. Statquest double bam.  Gray, we've made it to the end of another exciting. Statquest. If you like this stats Quest, and want to see more. Please subscribe. And if you want to support statquest, well, please consider buying one or two of my original songs. All right, until next time Quest on. 