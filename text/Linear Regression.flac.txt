Walking back. So we're talking about the SVG and how we can use it to solve linear systems of equations. AI x equal to be and now we're ready to talk about building. Linearregression model is from data. So this is really exciting because regression models are one of the best tools we have when we have measurement data to build predicted model that we can use for the future. And in some linear regression is kind of the the first step in lots of lots of approaches to learning. It's the first model you would try to build if you had data and some going to walk you through this. And we're going to talk about how how you would compute this. And we're actually going to run a couple of codec samples on real data to play around with these regressions model. So the idea might have some Matrix Ai, and I'm going to die as a tall skinny Matrix a X affect her ex would equal some output be. So a x equals be and what I do this in the over to term in case because often times that's what we have in modern data examples. So I want to I want to walk through like a very concrete example of what what this regression model might be. Lets say that every row of this, every row equation is based on data from an individual humans medical history in case. So every every row Is an individual person? Okay. This is let's say this is my ro-and. Every column of the a matrix are risk factors. Like you might have weights. Are you a smoker?  Etc. Etc. Gauge weight smoker diet, so on and so forth. And maybe the be back term is your risk of heart disease. So this is the risk of  heart disease.  And what we would like to do is from a large dataset where we actually follow maybe ten thousand people and we measure their their risk factors and then we, we figure out. Did they actually get heart disease at some point in the future. We actually have data for a + 4B. We would want to learn a best fit model X, for what combination of those factors describe best or predicted, best the future risk of heart disease can. So, for example, in this case, you know, you have your age or weight. I'm not a smoker etcetera. Etcetera. I'm multiple vector by acts, which is my model descent. Next time, with my model for how those risk factors translate into a probability were a risk of heart disease. I'm a massively oversimplifying here. But this is the basic idea of what we might want to do based on real measurement data. So I would actually measure the AI Matrix and the bee Matrix is my cost 50.  Dollars to go and do this large large, cardiac study to measure all of these variables for 10,000 people. But once I had that, I want to build the best possible model X. So that in the future. I have a new patient eye. Lan said, it happened to compute the risk of heart disease with my chi. Very complicated. I don't want to wait to see if they get it. I just asked him a few questions. I X this text Victor and I get a number which is hopefully predictive of their their future risk of heart disease and acts that exactly solved this model for every single person cuz it's over determined and noisy and messy, but we want to find the exit and minimizes the error Norm between LAX and it gives us the best predictor to be given information. In egg were using least-squares to solve these regressions models.  And I'm just going to draw some pictures here. We're going to cover this up on some real data. So I'm not going to be too precise here. But let's say that I have my ax, He's A and B. Sobe is the thing. I want to predict a is a measurement that I get to use to predict beads, and I'm just going to draw this in the one-dimensional case. So let's say I have a bunch of data that scattered around and it looks like this. So I asked you cuz I predicted you would say, it looks pretty good. It looks roughly. Linearregression.  I'm going to walk you through how to do this.  All of this regression generalize has two very high dimensional. Very big data in this Matrix system of equations. I maybe I have 100 doctors say that I'm going to use to predict be none of the tasks changes. So in general, I could have, you know, Factor one and I can have Factor 2 and Dr. 3 and 4, and 5, and I can build this instead of a best fit line. I can find a best fit plane that models be as a function of these input factors. The Columns of my a matrix. I'm going to keep it simple and I'm just going to assume that I have one factor a and I'm going to try to predicted be using that one factor.  And usually my data is going to come in the form of pairs of measurements, a four-person one, and be for person, one a 4 person too, and be for person too. And so on and so forth.  And I'm going to have a lot of these measurements and I'm going to stack them into these tall vectors of data is actually the data. I'm going to get is like what's a, this is a 1 b14. Personalized is a to be 24 person too and so on and so forth. Every person is a DOT on this, on this, the scatterplot and I'm going to arrange them like this and then I'm going to try to find the best bits slope X bitmap say in to be. And what's nice is that this is going to generalize. So that if we have multiple factors, multi affect your measurements, person one person to person 3, we can find the best bits slope X, the models, be  Good. And so in one Dimensions, this is quite simple. Essentially, in in one dimension, what we have, if I just compute the SVG of the same Matrix cuz that's how I'm going to do inverse. I'm going to get you.  Equals by a vector divided by its Norm, a divided by its Norm. I'm going to have Sigma equals just the norm of a  and because this is a one-dimensional matrix. It has just one columns. The V. Matrix is going to be trivial. It's just going to be 12v is going to equal one and it's really easy to compute, the pseudo inverse of this to solve for x using this formula here. And I'm just going to ride out with the answer, is the best fit x x Tilda that best fits this this software. This line is equal to a transpose be. So the inner product of my, a vector and I be Vector /, the norm of AI squared. That is my best fit slope.  Ai ai in it kind of makes sense. Right? What I'm going to do is I'm going to basically take be all of the be data and projected into the direction of AI. That's what this is. And then I'm going to normalize that a vector by its length. And I'm going to normalize the be back here by its length because I'm going to multiple that slope by a to get beat. So this is the best fit line. It's exactly what you would get. If you play these values into this pseudoinverse in this formula Chi here, but this formula generalize is too much higher dimensional data, where there are multiple risk factors and you want to build models. So I think that's pretty cool. Now. The last thing I'm going to point out is that in in data analysis often you have outliers. So this would have drawn, there's some variability. Maybe there's some, some ocean noise on top of this, this line because every person has a little bit of variability. There's not a perfect model for all people, but if I have a massive,  Liar down here. Someone just was completely different than everyone else. What that's normally going to do is it's going to completely bias. My distribution might my best bits Loop because it's trying to minimize the sum of the squares of the errors of all of these points to the line. And at this point is really far away than the square of that distance is much bigger than the square of all of these distances. And it's going to pull the whole distribution down. So this is the risk we have and we have outliers or corruption in the data. And so regularly squared errors based on the SVG can handle White Noise really, really well. In case if there's some Grouch & distribution of data, you're going to find the best slope, but if there are a few big the outliers, then this is actually going to be very very sensitive to those outliers. And so this is going to motivate a lot of what we're going to talk about in chapter three of our book on robustus.  6, compressed sensing sparsity and there are robust variance of this. So we're going to find robust.  Bits.  Where we add an L1 penalty term to kind of discount outliers. Guess we'll talk a lot about that in chapter 3. I just wanted to point out now that that is a risk of these outliers. But if you have relatively clean data or it only has kind of Goshen noise are captions distribution. Then this is actually going to be optimal for forfeiting. Your data is the singular value decomposition, follow section 1.4 in the book, everything here. And the next videos were going to codec up some examples of this to show you how to actually fit these things to show you. What happens when you out wires were multiple factors. Okay. Thank you. 