Welcome to byte size are assignments in the last four videos recovered algorithm stats required. Supervised learning ensembles and logistic regression. Let's take a break from it. And look at some unsupervised learning algorithms algorithm for cluster ring. Cluster. Ring is about drooping. Similar objects. For example, similar individuals may have similar tastes and needs similar products and possibly be interchangeable. Do they need of a customer? What do we mean by similar? For example, for individuals? We can look at age gender income. And so, it is pretty obvious that what appeals to a 20-year old is quite different from what appeals to 60 year old. We can group, people objects events, music movies location, and just about anything. We can think of.  If you think of buying a house, you can look at the number of bathrooms. And number of bedrooms. If it is a condo apartment duplex and so on, you can routing neighborhoods by characteristics and then find multiple houses in similar neighborhoods that can answer the needs of a buyer.  Similarity is a measure of distance between objects. One Chi 2 plus training is to figure out how to express the characteristics in terms of numbers that say we want the group hair. Dye by color. It is not obvious. How close is brown and Auburn are. But if you come for the colors to an RGB representation, we now have three numbers that can be used for similarity.  What are the algorithm is available for cluster? And there are many algorithms available? For example, here's a list of algorithms that are part of the escalator in library.  There are multiple approaches to Cluster ring. We can use a top-down or bottom-up approach. For example, do we know how many classes we want? We also have to consider the number of data points to processor. And I was specifically Gordon performance. We will only cover a few algorithms here, starting with. Tammy's K means is probably the best-known cluster node or then and probably the easiest to understand Chi stats for the number of clusters. We want, here's an example of how it works, the animation on my right shows the entire process. The first step is to pick a starting points.  The easiest task to pick Tiga, sync points as cluster centers in the upper-left image. We picked the Four Points. Farthest to the left. The next step is to associate all the point to these clusters centers. As shown in the second image note, that this process left one center with no Associated points. Once this is done. We re sentiment of clusters and repeat the process until a number of iterations is done. Or if descent their movements are blue, a decided threshold.  The choice of starting points impacts the result. So it makes sense to repeat the process with other centres. When we looked at the starting image. We may have visual eyes. Different cluster is than the ones we ended up with?  How do we decide on the number cluster? We can try a different number cluster and CD impact of the distance from the cluster centers in the diagram. Here. We see a quick drop at the start increasing the number of Fosters. This is called the elbow method. We pick evaluate where the curve is. Another approach is to build a cluster is from the bottom up. I'm here shows the number of clusters and now they would group to reduce their numbers. This was done using the AI deliberative cluster in algorithms. The bottom layer of the right of the diagram shows, 50 plus there is, and how they would group together to merge together.  The last algorithm I want to mention is DB scan, which stands for density. Bayes special, cluster ring affect relations with noise.  With this algorithm, we can eliminate outliers. So they don't impact our cluster ring the image on my right shows two clusters. Each cluster different shades between the readings that are in a dance-off core area, and the ones that are still part of the cluster is, but I'm not part of the core samples. Finally. We see the outliers values. The great. That are not part of a cluster. And archive, in storing noise. The number of clusters is automatically decided based on density, parmiter's  In the next video. We'll see, I use Chi stats gation of these algorithms. See you next time. I'm by Ties That a science and don't forget to subscribe. 